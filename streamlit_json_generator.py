"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ JSON (–æ–¥–∏–Ω —Ñ–∞–π–ª)
=======================================
–ü—Ä–æ—Å—Ç–æ–π Streamlit-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Product Offering Group –∏ Category.
–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π UI + –¥–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –≤—Å–µ—Ö –æ—à–∏–±–æ–∫ –∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤.
"""

import io
import json
import zipfile
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Tuple, Optional
from enum import Enum

import pandas as pd
import streamlit as st

# =========================
# –ö–û–ù–°–¢–ê–ù–¢–´
# =========================
DEFAULT_LOCALE = "en-US"
POG_DIR = "productOfferingGroup"
POC_DIR = "productOfferingCategory"

SAFE_NAME_PATTERN = re.compile(r"[^0-9A-Za-z_\-\u0400-\u04FF]")
WHITESPACE_PATTERN = re.compile(r"\s+")


# =========================
# –¢–ò–ü–´ –ü–†–û–ë–õ–ï–ú
# =========================
class IssueType(Enum):
    ALREADY_EXISTS = "already_exists"
    ALREADY_EXPIRED = "already_expired"
    DUPLICATE_IN_SOURCE = "duplicate_in_source"
    NOT_FOUND_JSON_ID = "not_found_json_id"
    NOT_FOUND_SERVICE_ID = "not_found_service_id"
    NOT_FOUND_OFFER_ID = "not_found_offer_id"
    INVALID_TARGET_TYPE = "invalid_target_type"
    EMPTY_ID = "empty_id"
    INVALID_JSON = "invalid_json"
    MISSING_FIELD = "missing_field"


@dataclass
class Issue:
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ –∏–ª–∏ –ø—Ä–æ–ø—É—Å–∫–µ"""
    type: IssueType
    severity: str  # "warning", "error", "info"
    message: str
    context: Dict[str, Any] = field(default_factory=dict)
    row_number: Optional[int] = None
    file_path: Optional[str] = None


@dataclass
class SimpleResult:
    ok: bool
    msg: str
    zip_data: Optional[io.BytesIO]
    counts: Dict[str, int]
    issues: List[Issue] = field(default_factory=list)
    details: Optional[Dict[str, Any]] = None
    
    def add_issue(self, issue: Issue):
        self.issues.append(issue)


# =========================
# –£–¢–ò–õ–ò–¢–´
# =========================
def _safe_name(name: str) -> str:
    if not isinstance(name, str):
        name = str(name)
    s = WHITESPACE_PATTERN.sub("_", name.strip())
    s = SAFE_NAME_PATTERN.sub("", s)
    return s or "file"


def _normalize_str(v: Any) -> str:
    if pd.isna(v):
        return ""
    s = str(v).strip()
    return "" if s.lower() == "nan" else s


def _normalize_id(v: Any) -> str:
    s = _normalize_str(v)
    return s if s else ""


def _json_dumps_stable(obj: Any) -> str:
    return json.dumps(obj, ensure_ascii=False, indent=4, sort_keys=True)


def _read_table(excel_bytes: bytes, expected_cols: List[str]) -> Tuple[pd.DataFrame, List[Issue]]:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∏–¥–µ—Ä —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º"""
    issues = []
    buf = io.BytesIO(excel_bytes)

    try:
        df = pd.read_excel(buf, engine="openpyxl")
    except Exception as e:
        issues.append(Issue(
            type=IssueType.INVALID_JSON,
            severity="info",
            message=f"–ù–µ Excel, –ø—Ä–æ–±—É–µ–º CSV: {str(e)[:50]}"
        ))
        buf.seek(0)
        try:
            df = pd.read_csv(buf)
        except Exception:
            buf.seek(0)
            try:
                df = pd.read_csv(buf, sep=";", engine="python")
            except Exception as e2:
                issues.append(Issue(
                    type=IssueType.INVALID_JSON,
                    severity="error",
                    message=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {str(e2)}"
                ))
                raise

    missing = [c for c in expected_cols if c not in df.columns]
    if missing:
        issues.append(Issue(
            type=IssueType.MISSING_FIELD,
            severity="error",
            message=f"–ù–µ—Ç —Å—Ç–æ–ª–±—Ü–∞(–æ–≤): {', '.join(missing)}",
            context={"missing": missing, "available": list(df.columns)}
        ))
        raise KeyError(f"–ù–µ—Ç —Ç—Ä–µ–±—É–µ–º–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞(–æ–≤): {', '.join(missing)}")

    return df[expected_cols].copy(), issues


# =========================
# ZIP/JSON I/O
# =========================
def _read_zip(zip_bytes: bytes) -> Tuple[List[str], Dict[str, bytes], List[Issue]]:
    issues = []
    try:
        with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as zf:
            names = zf.namelist()
            content = {n: zf.read(n) for n in names}
        return names, content, issues
    except Exception as e:
        issues.append(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è ZIP: {str(e)}"
        ))
        raise


def _list_json_in_dir(bytes_map: Dict[str, bytes], dir_name: str) -> List[str]:
    prefix = f"{dir_name}/"
    return [n for n in bytes_map if n.startswith(prefix) and n.endswith(".json")]


def _load_json(data: bytes, path: str, issues: List[Issue]) -> Optional[Dict[str, Any]]:
    try:
        return json.loads(data.decode("utf-8"))
    except Exception as e:
        issues.append(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON",
            file_path=path,
            context={"error": str(e)[:100]}
        ))
        return None


def _build_new_zip(original_names: List[str], original_bytes: Dict[str, bytes],
                   updated_json_map: Dict[str, str]) -> io.BytesIO:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for name in original_names:
            if name in updated_json_map:
                zf.writestr(name, updated_json_map[name].encode("utf-8"))
            else:
                zf.writestr(name, original_bytes[name])
    buf.seek(0)
    return buf


# =========================
# BUILDERS
# =========================
def _make_offering(offer_id: str, name: Optional[str] = None,
                   locale: str = DEFAULT_LOCALE, expired: bool = False) -> Dict[str, Any]:
    item: Dict[str, Any] = {
        "id": offer_id,
        "isBundle": False,
        "expiredForSales": expired
    }
    if name:
        item["name"] = [{"locale": locale, "value": name}]
    return item


def _build_pog_addon(json_name: str, json_id: str, locale: str,
                     offerings: List[Dict[str, Any]]) -> Dict[str, Any]:
    return {
        "effective": True,
        "externalId": [],
        "id": json_id,
        "localizedName": [{"locale": locale, "value": json_name}],
        "name": _safe_name(json_name),
        "policy": [],
        "productOfferingsInGroup": sorted(offerings, key=lambda x: x["id"]),
        "purpose": ["addOn"],
        "restriction": []
    }


def _build_pog_replace(json_name: str, json_id: str, locale: str,
                       offerings: List[Dict[str, Any]]) -> Dict[str, Any]:
    return {
        "description": [{"locale": locale, "value": json_name}],
        "effective": True,
        "externalId": [],
        "id": json_id,
        "localizedName": [{"locale": locale, "value": json_name}],
        "name": _safe_name(json_name),
        "policy": [],
        "productOfferingsInGroup": sorted(offerings, key=lambda x: x["id"]),
        "purpose": ["replaceOffer"],
        "restriction": []
    }


def _build_category(offer_id: str, category_ids: List[str]) -> Dict[str, Any]:
    unique_sorted = sorted({cid for cid in (_normalize_id(c) for c in category_ids) if cid})
    return {
        "id": offer_id,
        "category": unique_sorted,
        "categoryRef": [{"id": cid} for cid in unique_sorted]
    }


# =========================
# –û–ü–ï–†–ê–¶–ò–ò
# =========================
def generate_addon_from_excel(excel_bytes: bytes) -> SimpleResult:
    """1. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É—Å–ª—É–≥–∏ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞—Ä–∏—Ñ–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤."""
    result = SimpleResult(False, "", None, {})
    
    try:
        expected = ["Addons name", "Addons ID", "–ò–º—è —É—Å–ª—É–≥–∏", "ID —É—Å–ª—É–≥–∏"]
        df, read_issues = _read_table(excel_bytes, expected)
        result.issues.extend(read_issues)
        
        for c in expected:
            df[c] = df[c].apply(_normalize_str)
        
        total_rows = len(df)
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö ID
        for idx, row in df.iterrows():
            if not row["Addons ID"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π Addons ID",
                    row_number=idx + 2,
                    context={"addons_name": row["Addons name"]}
                ))
            if not row["ID —É—Å–ª—É–≥–∏"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π ID —É—Å–ª—É–≥–∏",
                    row_number=idx + 2,
                    context={"service_name": row["–ò–º—è —É—Å–ª—É–≥–∏"]}
                ))
        
        df = df[(df["Addons ID"] != "") & (df["ID —É—Å–ª—É–≥–∏"] != "")]
        
        if df.empty:
            result.msg = "–í Excel –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫"
            return result
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(df)
        result.counts["skipped_rows"] = total_rows - len(df)
        
        groups = df.groupby(["Addons name", "Addons ID"])
        buf = io.BytesIO()
        created_jsons = 0
        services_total = 0
        
        with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
            for (json_name, json_id), g in groups:
                initial_count = len(g)
                g = g.drop_duplicates(subset=["ID —É—Å–ª—É–≥–∏"])
                duplicates_count = initial_count - len(g)
                
                if duplicates_count > 0:
                    result.add_issue(Issue(
                        type=IssueType.DUPLICATE_IN_SOURCE,
                        severity="info",
                        message=f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count}",
                        context={"addons_id": json_id, "addons_name": json_name}
                    ))
                
                offerings = []
                for _, r in g.iterrows():
                    sid = _normalize_id(r["ID —É—Å–ª—É–≥–∏"])
                    sname = _normalize_str(r["–ò–º—è —É—Å–ª—É–≥–∏"])
                    if not sid:
                        continue
                    offerings.append(_make_offering(sid, sname, DEFAULT_LOCALE))
                
                if not offerings:
                    continue
                
                pog = _build_pog_addon(_normalize_str(json_name), _normalize_id(json_id), DEFAULT_LOCALE, offerings)
                zf.writestr(f"{POG_DIR}/{_safe_name(json_id)}.json", _json_dumps_stable(pog))
                created_jsons += 1
                services_total += len(offerings)
        
        if created_jsons == 0:
            result.msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ JSON"
            return result
        
        result.counts["created_jsons"] = created_jsons
        result.counts["services_total"] = services_total
        buf.seek(0)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def add_services_to_existing_pogs(zip_bytes: bytes, excel_bytes: bytes) -> SimpleResult:
    """2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–ª–∞–Ω—ã."""
    result = SimpleResult(False, "", None, {})
    
    try:
        names, blob, zip_issues = _read_zip(zip_bytes)
        result.issues.extend(zip_issues)
        
        json_files = _list_json_in_dir(blob, POG_DIR)
        if not json_files:
            result.msg = f"–í ZIP –Ω–µ—Ç JSON –≤ {POG_DIR}/"
            return result
        
        result.counts["json_files_in_zip"] = len(json_files)
        
        expected = ["Addons ID", "–ò–º—è —É—Å–ª—É–≥–∏", "ID —É—Å–ª—É–≥–∏"]
        df, read_issues = _read_table(excel_bytes, expected)
        result.issues.extend(read_issues)
        
        total_rows = len(df)
        
        for c in expected:
            df[c] = df[c].apply(_normalize_str)
        
        for idx, row in df.iterrows():
            if not row["Addons ID"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π Addons ID",
                    row_number=idx + 2
                ))
            if not row["ID —É—Å–ª—É–≥–∏"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π ID —É—Å–ª—É–≥–∏",
                    row_number=idx + 2
                ))
        
        df = df[(df["Addons ID"] != "") & (df["ID —É—Å–ª—É–≥–∏"] != "")]
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(df)
        
        service_map = df.groupby("Addons ID")[["–ò–º—è —É—Å–ª—É–≥–∏", "ID —É—Å–ª—É–≥–∏"]].apply(lambda x: x.to_dict("records")).to_dict()
        
        updated: Dict[str, str] = {}
        found_ids = set()
        skipped_rows: List[Dict[str, str]] = []
        
        for path in json_files:
            data = _load_json(blob[path], path, result.issues)
            if not data:
                continue
            
            json_id = _normalize_id(data.get("id", ""))
            if not json_id:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="error",
                    message="JSON –±–µ–∑ ID",
                    file_path=path
                ))
                continue
            
            if json_id not in service_map:
                continue
            
            found_ids.add(json_id)
            
            if data.get("purpose") != ["addOn"]:
                result.add_issue(Issue(
                    type=IssueType.INVALID_TARGET_TYPE,
                    severity="error",
                    message=f"–ù–µ–≤–µ—Ä–Ω—ã–π purpose (–æ–∂–∏–¥–∞–µ—Ç—Å—è addOn)",
                    file_path=path,
                    context={"json_id": json_id, "purpose": data.get("purpose")}
                ))
                continue
            
            offerings = data.get("productOfferingsInGroup", [])
            existing = {_normalize_id(o.get("id", "")) for o in offerings}
            
            modified = False
            for rec in service_map[json_id]:
                sid = _normalize_id(rec["ID —É—Å–ª—É–≥–∏"])
                sname = _normalize_str(rec["–ò–º—è —É—Å–ª—É–≥–∏"])
                if not sid:
                    continue
                
                if sid in existing:
                    result.add_issue(Issue(
                        type=IssueType.ALREADY_EXISTS,
                        severity="info",
                        message=f"–£—Å–ª—É–≥–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç",
                        file_path=path,
                        context={"json_id": json_id, "service_id": sid, "service_name": sname}
                    ))
                    skipped_rows.append({
                        "json_id": json_id,
                        "service_id": sid,
                        "service_name": sname,
                        "reason": "already_exists_in_group"
                    })
                else:
                    offerings.append(_make_offering(sid, sname, DEFAULT_LOCALE))
                    existing.add(sid)
                    modified = True
            
            if modified:
                data["productOfferingsInGroup"] = sorted(offerings, key=lambda x: x["id"])
                updated[path] = _json_dumps_stable(data)
        
        for want_id in service_map.keys():
            if want_id not in found_ids:
                result.add_issue(Issue(
                    type=IssueType.NOT_FOUND_JSON_ID,
                    severity="error",
                    message=f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                    context={"addons_id": want_id}
                ))
        
        result.counts["files_processed"] = len(updated)
        result.counts["added"] = sum(1 for i in result.issues if i.type == IssueType.ALREADY_EXISTS)
        result.counts["skipped_existing"] = len(skipped_rows)
        result.details = {"skipped_existing": skipped_rows}
        
        if not updated:
            result.ok = True
            result.msg = "–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"
            return result
        
        buf = _build_new_zip(names, blob, updated)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def expire_services_in_pogs(zip_bytes: bytes, excel_bytes: bytes) -> SimpleResult:
    """3. –≠–∫—Å–ø–∞–π—Ä —É—Å–ª—É–≥–∏."""
    result = SimpleResult(False, "", None, {})
    
    try:
        names, blob, zip_issues = _read_zip(zip_bytes)
        result.issues.extend(zip_issues)
        
        json_files = _list_json_in_dir(blob, POG_DIR)
        if not json_files:
            result.msg = f"–í ZIP –Ω–µ—Ç JSON –≤ {POG_DIR}/"
            return result
        
        result.counts["json_files_in_zip"] = len(json_files)
        
        df, read_issues = _read_table(excel_bytes, ["json_id", "service_id"])
        result.issues.extend(read_issues)
        
        total_rows = len(df)
        
        for c in ["json_id", "service_id"]:
            df[c] = df[c].apply(_normalize_str)
        
        for idx, row in df.iterrows():
            if not row["json_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π json_id",
                    row_number=idx + 2
                ))
            if not row["service_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π service_id",
                    row_number=idx + 2
                ))
        
        df = df[(df["json_id"] != "") & (df["service_id"] != "")]
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(df)
        
        expire_map = df.groupby("json_id")["service_id"].apply(list).to_dict()
        
        updated: Dict[str, str] = {}
        found_ids = set()
        
        for path in json_files:
            data = _load_json(blob[path], path, result.issues)
            if not data:
                continue
            
            json_id = _normalize_id(data.get("id", ""))
            if not json_id or json_id not in expire_map:
                continue
            
            found_ids.add(json_id)
            
            if data.get("purpose") != ["addOn"]:
                result.add_issue(Issue(
                    type=IssueType.INVALID_TARGET_TYPE,
                    severity="error",
                    message=f"–ù–µ–≤–µ—Ä–Ω—ã–π purpose (–æ–∂–∏–¥–∞–µ—Ç—Å—è addOn)",
                    file_path=path,
                    context={"json_id": json_id}
                ))
                continue
            
            offerings = data.get("productOfferingsInGroup", [])
            index_by_id = {_normalize_id(o.get("id", "")): o for o in offerings}
            
            modified = False
            for sid in expire_map[json_id]:
                sid = _normalize_id(sid)
                o = index_by_id.get(sid)
                if o is None:
                    result.add_issue(Issue(
                        type=IssueType.NOT_FOUND_SERVICE_ID,
                        severity="error",
                        message=f"–£—Å–ª—É–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                        file_path=path,
                        context={"json_id": json_id, "service_id": sid}
                    ))
                    continue
                
                if not o.get("expiredForSales", False):
                    o["expiredForSales"] = True
                    modified = True
                else:
                    result.add_issue(Issue(
                        type=IssueType.ALREADY_EXPIRED,
                        severity="info",
                        message=f"–£—Å–ª—É–≥–∞ —É–∂–µ —ç–∫—Å–ø–∞–π—Ä–Ω—É—Ç–∞",
                        file_path=path,
                        context={"json_id": json_id, "service_id": sid}
                    ))
            
            if modified:
                data["productOfferingsInGroup"] = sorted(offerings, key=lambda x: x["id"])
                updated[path] = _json_dumps_stable(data)
        
        for want_id in expire_map.keys():
            if want_id not in found_ids:
                result.add_issue(Issue(
                    type=IssueType.NOT_FOUND_JSON_ID,
                    severity="error",
                    message=f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                    context={"json_id": want_id}
                ))
        
        result.counts["files_processed"] = len(updated)
        result.counts["expired"] = sum(1 for i in result.issues if i.type == IssueType.ALREADY_EXPIRED)
        
        if not updated:
            result.ok = True
            result.msg = "–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"
            return result
        
        buf = _build_new_zip(names, blob, updated)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def expire_and_add_services(zip_bytes: bytes, expire_excel: bytes, add_excel: bytes) -> SimpleResult:
    """4. –≠–∫—Å–ø–∞–π—Ä + –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏ (–¥–≤–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏)."""
    result = SimpleResult(False, "", None, {})
    
    try:
        # –ß–∏—Ç–∞–µ–º ZIP
        names, blob, zip_issues = _read_zip(zip_bytes)
        result.issues.extend(zip_issues)
        
        json_files = _list_json_in_dir(blob, POG_DIR)
        if not json_files:
            result.msg = f"–í ZIP –Ω–µ—Ç JSON –≤ {POG_DIR}/"
            return result
        
        result.counts["json_files_in_zip"] = len(json_files)
        
        # === –≠–¢–ê–ü 1: –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞ ===
        df_expire, expire_issues = _read_table(expire_excel, ["ID —É—Å–ª—É–≥–∏", "–ò–º—è —É—Å–ª—É–≥–∏"])
        result.issues.extend(expire_issues)
        
        total_expire_rows = len(df_expire)
        
        for c in ["ID —É—Å–ª—É–≥–∏", "–ò–º—è —É—Å–ª—É–≥–∏"]:
            df_expire[c] = df_expire[c].apply(_normalize_str)
        
        for idx, row in df_expire.iterrows():
            if not row["ID —É—Å–ª—É–≥–∏"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π ID —É—Å–ª—É–≥–∏ –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞",
                    row_number=idx + 2
                ))
        
        df_expire = df_expire[df_expire["ID —É—Å–ª—É–≥–∏"] != ""]
        
        result.counts["expire_total_rows"] = total_expire_rows
        result.counts["expire_valid_rows"] = len(df_expire)
        
        # –°–æ–∑–¥–∞–µ–º set –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        services_to_expire = {_normalize_id(row["ID —É—Å–ª—É–≥–∏"]) for _, row in df_expire.iterrows()}
        
        # === –≠–¢–ê–ü 2: –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è ===
        df_add, add_issues = _read_table(add_excel, ["ID —É—Å–ª—É–≥–∏", "–ò–º—è —É—Å–ª—É–≥–∏"])
        result.issues.extend(add_issues)
        
        total_add_rows = len(df_add)
        
        for c in ["ID —É—Å–ª—É–≥–∏", "–ò–º—è —É—Å–ª—É–≥–∏"]:
            df_add[c] = df_add[c].apply(_normalize_str)
        
        for idx, row in df_add.iterrows():
            if not row["ID —É—Å–ª—É–≥–∏"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π ID —É—Å–ª—É–≥–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è",
                    row_number=idx + 2
                ))
        
        df_add = df_add[df_add["ID —É—Å–ª—É–≥–∏"] != ""]
        
        result.counts["add_total_rows"] = total_add_rows
        result.counts["add_valid_rows"] = len(df_add)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (warning)
        services_to_add_ids = {_normalize_id(row["ID —É—Å–ª—É–≥–∏"]) for _, row in df_add.iterrows()}
        overlap = services_to_expire & services_to_add_ids
        if overlap:
            result.add_issue(Issue(
                type=IssueType.DUPLICATE_IN_SOURCE,
                severity="warning",
                message=f"–£—Å–ª—É–≥–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –æ–±–æ–∏—Ö —Ñ–∞–π–ª–∞—Ö: {', '.join(list(overlap)[:5])}",
                context={"overlap_count": len(overlap)}
            ))
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —É—Å–ª—É–≥ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å –∏–º–µ–Ω–∞–º–∏
        services_to_add = []
        for _, row in df_add.iterrows():
            sid = _normalize_id(row["ID —É—Å–ª—É–≥–∏"])
            sname = _normalize_str(row["–ò–º—è —É—Å–ª—É–≥–∏"])
            if sid:
                services_to_add.append({"id": sid, "name": sname})
        
        # === –≠–¢–ê–ü 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON —Ñ–∞–π–ª–æ–≤ ===
        updated: Dict[str, str] = {}
        expired_count = 0
        added_count = 0
        skipped_expire_not_found = []
        skipped_add_existing = []
        
        for path in json_files:
            data = _load_json(blob[path], path, result.issues)
            if not data:
                continue
            
            json_id = _normalize_id(data.get("id", ""))
            if not json_id:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="error",
                    message="JSON –±–µ–∑ ID",
                    file_path=path
                ))
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º purpose
            if data.get("purpose") != ["addOn"]:
                result.add_issue(Issue(
                    type=IssueType.INVALID_TARGET_TYPE,
                    severity="error",
                    message=f"–ù–µ–≤–µ—Ä–Ω—ã–π purpose (–æ–∂–∏–¥–∞–µ—Ç—Å—è addOn)",
                    file_path=path,
                    context={"json_id": json_id, "purpose": data.get("purpose")}
                ))
                continue
            
            offerings = data.get("productOfferingsInGroup", [])
            existing_ids = {_normalize_id(o.get("id", "")) for o in offerings}
            modified = False
            
            # --- –û–ø–µ—Ä–∞—Ü–∏—è 1: –≠–∫—Å–ø–∞–π—Ä ---
            for offering in offerings:
                sid = _normalize_id(offering.get("id", ""))
                if sid in services_to_expire:
                    if not offering.get("expiredForSales", False):
                        offering["expiredForSales"] = True
                        expired_count += 1
                        modified = True
                    else:
                        result.add_issue(Issue(
                            type=IssueType.ALREADY_EXPIRED,
                            severity="info",
                            message=f"–£—Å–ª—É–≥–∞ —É–∂–µ —ç–∫—Å–ø–∞–π—Ä–Ω—É—Ç–∞",
                            file_path=path,
                            context={"json_id": json_id, "service_id": sid}
                        ))
            
            # --- –û–ø–µ—Ä–∞—Ü–∏—è 2: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ---
            for service in services_to_add:
                sid = service["id"]
                sname = service["name"]
                
                if sid in existing_ids:
                    skipped_add_existing.append({
                        "json_id": json_id,
                        "service_id": sid,
                        "service_name": sname,
                        "reason": "already_exists"
                    })
                    result.add_issue(Issue(
                        type=IssueType.ALREADY_EXISTS,
                        severity="info",
                        message=f"–£—Å–ª—É–≥–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç",
                        file_path=path,
                        context={"json_id": json_id, "service_id": sid, "service_name": sname}
                    ))
                else:
                    offerings.append(_make_offering(sid, sname, DEFAULT_LOCALE))
                    existing_ids.add(sid)
                    added_count += 1
                    modified = True
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if modified:
                data["productOfferingsInGroup"] = sorted(offerings, key=lambda x: x["id"])
                updated[path] = _json_dumps_stable(data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —É—Å–ª—É–≥–∏ –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞ –Ω–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã
        found_expired = set()
        for path in json_files:
            data = _load_json(blob[path], path, [])
            if data:
                offerings = data.get("productOfferingsInGroup", [])
                for o in offerings:
                    sid = _normalize_id(o.get("id", ""))
                    if sid in services_to_expire:
                        found_expired.add(sid)
        
        not_found_expire = services_to_expire - found_expired
        for sid in not_found_expire:
            skipped_expire_not_found.append({
                "service_id": sid,
                "reason": "not_found_in_any_json"
            })
            result.add_issue(Issue(
                type=IssueType.NOT_FOUND_SERVICE_ID,
                severity="info",
                message=f"–£—Å–ª—É–≥–∞ –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –≤ –æ–¥–Ω–æ–º JSON",
                context={"service_id": sid}
            ))
        
        # === –≠–¢–ê–ü 4: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ===
        result.counts["files_processed"] = len(updated)
        result.counts["services_expired"] = expired_count
        result.counts["services_added"] = added_count
        result.counts["skipped_expire_not_found"] = len(skipped_expire_not_found)
        result.counts["skipped_add_existing"] = len(skipped_add_existing)
        
        result.details = {
            "skipped_expire_not_found": skipped_expire_not_found,
            "skipped_add_existing": skipped_add_existing
        }
        
        if not updated:
            result.ok = True
            result.msg = "–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"
            return result
        
        buf = _build_new_zip(names, blob, updated)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def create_replace_offer_from_excel(excel_bytes: bytes, json_name: str, json_id: str) -> SimpleResult:
    """1. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∞—Ä–∏—Ñ–Ω–æ–≥–æ –ø–ª–∞–Ω–∞."""
    result = SimpleResult(False, "", None, {})
    
    try:
        df, read_issues = _read_table(excel_bytes, ["offer_id"])
        result.issues.extend(read_issues)
        
        total_rows = len(df)
        df["offer_id"] = df["offer_id"].apply(_normalize_str)
        
        for idx, row in df.iterrows():
            if not row["offer_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π offer_id",
                    row_number=idx + 2
                ))
        
        df = df[df["offer_id"] != ""]
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(df)
        
        if df.empty:
            result.msg = "–í Excel –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫"
            return result
        
        offers = [_make_offering(_normalize_id(r["offer_id"])) for _, r in df.iterrows() if _normalize_id(r["offer_id"])]
        pog = _build_pog_replace(_normalize_str(json_name), _normalize_id(json_id), DEFAULT_LOCALE, offers)
        
        buf = io.BytesIO()
        with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
            zf.writestr(f"{POG_DIR}/{_safe_name(json_id)}.json", _json_dumps_stable(pog))
        buf.seek(0)
        
        result.counts["created_jsons"] = 1
        result.counts["offers_total"] = len(offers)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def add_offer_to_transitions(zip_bytes: bytes, excel_bytes: bytes, offer_id: str) -> SimpleResult:
    """2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–∞—Ä–∏—Ñ–∞ –≤ –ø–µ—Ä–µ—Ö–æ–¥—ã."""
    result = SimpleResult(False, "", None, {})
    
    try:
        names, blob, zip_issues = _read_zip(zip_bytes)
        result.issues.extend(zip_issues)
        
        json_files = _list_json_in_dir(blob, POG_DIR)
        if not json_files:
            result.msg = f"–í ZIP –Ω–µ—Ç JSON –≤ {POG_DIR}/"
            return result
        
        result.counts["json_files_in_zip"] = len(json_files)
        
        df, read_issues = _read_table(excel_bytes, ["json_id"])
        result.issues.extend(read_issues)
        
        total_rows = len(df)
        df["json_id"] = df["json_id"].apply(_normalize_str)
        
        for idx, row in df.iterrows():
            if not row["json_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π json_id",
                    row_number=idx + 2
                ))
        
        target_ids = {x for x in df["json_id"].tolist() if x}
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(target_ids)
        
        updated: Dict[str, str] = {}
        seen = set()
        want = _normalize_id(offer_id)
        skipped_rows: List[Dict[str, str]] = []
        
        for path in json_files:
            data = _load_json(blob[path], path, result.issues)
            if not data:
                continue
            
            jid = _normalize_id(data.get("id", ""))
            if not jid or jid not in target_ids:
                continue
            
            seen.add(jid)
            
            if data.get("purpose") != ["replaceOffer"]:
                result.add_issue(Issue(
                    type=IssueType.INVALID_TARGET_TYPE,
                    severity="error",
                    message=f"–ù–µ–≤–µ—Ä–Ω—ã–π purpose (–æ–∂–∏–¥–∞–µ—Ç—Å—è replaceOffer)",
                    file_path=path,
                    context={"json_id": jid}
                ))
                continue
            
            offerings = data.get("productOfferingsInGroup", [])
            existing = {_normalize_id(o.get("id", "")) for o in offerings}
            
            if want in existing:
                result.add_issue(Issue(
                    type=IssueType.ALREADY_EXISTS,
                    severity="info",
                    message=f"–¢–∞—Ä–∏—Ñ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç",
                    file_path=path,
                    context={"json_id": jid, "offer_id": want}
                ))
                skipped_rows.append({
                    "json_id": jid,
                    "offer_id": want,
                    "reason": "already_exists_in_group"
                })
                continue
            
            offerings.append(_make_offering(want))
            data["productOfferingsInGroup"] = sorted(offerings, key=lambda x: x["id"])
            updated[path] = _json_dumps_stable(data)
        
        for want_id in target_ids:
            if want_id not in seen:
                result.add_issue(Issue(
                    type=IssueType.NOT_FOUND_JSON_ID,
                    severity="error",
                    message=f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                    context={"json_id": want_id}
                ))
        
        result.counts["files_processed"] = len(updated)
        result.counts["added"] = len(updated)
        result.counts["skipped_existing"] = len(skipped_rows)
        result.details = {"skipped_existing": skipped_rows}
        
        if not updated:
            result.ok = True
            result.msg = "–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"
            return result
        
        buf = _build_new_zip(names, blob, updated)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def expire_offer_in_transitions(zip_bytes: bytes, excel_bytes: bytes) -> SimpleResult:
    """3. –≠–∫—Å–ø–∞–π—Ä —Ç–∞—Ä–∏—Ñ–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ –≤ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö."""
    result = SimpleResult(False, "", None, {})
    
    try:
        names, blob, zip_issues = _read_zip(zip_bytes)
        result.issues.extend(zip_issues)
        
        json_files = _list_json_in_dir(blob, POG_DIR)
        if not json_files:
            result.msg = f"–í ZIP –Ω–µ—Ç JSON –≤ {POG_DIR}/"
            return result
        
        result.counts["json_files_in_zip"] = len(json_files)
        
        df, read_issues = _read_table(excel_bytes, ["json_id", "offer_id"])
        result.issues.extend(read_issues)
        
        total_rows = len(df)
        
        for c in ["json_id", "offer_id"]:
            df[c] = df[c].apply(_normalize_str)
        
        for idx, row in df.iterrows():
            if not row["json_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π json_id",
                    row_number=idx + 2
                ))
            if not row["offer_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π offer_id",
                    row_number=idx + 2
                ))
        
        df = df[(df["json_id"] != "") & (df["offer_id"] != "")]
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(df)
        
        expire_map = df.groupby("json_id")["offer_id"].apply(list).to_dict()
        
        updated: Dict[str, str] = {}
        found_ids = set()
        
        for path in json_files:
            data = _load_json(blob[path], path, result.issues)
            if not data:
                continue
            
            jid = _normalize_id(data.get("id", ""))
            if not jid or jid not in expire_map:
                continue
            
            found_ids.add(jid)
            
            if data.get("purpose") != ["replaceOffer"]:
                result.add_issue(Issue(
                    type=IssueType.INVALID_TARGET_TYPE,
                    severity="error",
                    message=f"–ù–µ–≤–µ—Ä–Ω—ã–π purpose (–æ–∂–∏–¥–∞–µ—Ç—Å—è replaceOffer)",
                    file_path=path,
                    context={"json_id": jid}
                ))
                continue
            
            offerings = data.get("productOfferingsInGroup", [])
            index_by_id = {_normalize_id(o.get("id", "")): o for o in offerings}
            
            modified = False
            for oid in expire_map[jid]:
                oid = _normalize_id(oid)
                o = index_by_id.get(oid)
                if o is None:
                    result.add_issue(Issue(
                        type=IssueType.NOT_FOUND_OFFER_ID,
                        severity="error",
                        message=f"–¢–∞—Ä–∏—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω",
                        file_path=path,
                        context={"json_id": jid, "offer_id": oid}
                    ))
                    continue
                
                if not o.get("expiredForSales", False):
                    o["expiredForSales"] = True
                    modified = True
                else:
                    result.add_issue(Issue(
                        type=IssueType.ALREADY_EXPIRED,
                        severity="info",
                        message=f"–¢–∞—Ä–∏—Ñ —É–∂–µ —ç–∫—Å–ø–∞–π—Ä–Ω—É—Ç",
                        file_path=path,
                        context={"json_id": jid, "offer_id": oid}
                    ))
            
            if modified:
                data["productOfferingsInGroup"] = sorted(offerings, key=lambda x: x["id"])
                updated[path] = _json_dumps_stable(data)
        
        for want_id in expire_map.keys():
            if want_id not in found_ids:
                result.add_issue(Issue(
                    type=IssueType.NOT_FOUND_JSON_ID,
                    severity="error",
                    message=f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                    context={"json_id": want_id}
                ))
        
        result.counts["files_processed"] = len(updated)
        result.counts["expired"] = sum(1 for i in result.issues if i.type == IssueType.ALREADY_EXPIRED)
        
        if not updated:
            result.ok = True
            result.msg = "–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"
            return result
        
        buf = _build_new_zip(names, blob, updated)
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


def generate_categories_from_excel(excel_bytes: bytes) -> SimpleResult:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (ProductOfferingCategory)."""
    result = SimpleResult(False, "", None, {})
    
    try:
        df, read_issues = _read_table(excel_bytes, ["offer_id", "category_id"])
        result.issues.extend(read_issues)
        
        total_rows = len(df)
        
        for c in ["offer_id", "category_id"]:
            df[c] = df[c].apply(_normalize_str)
        
        for idx, row in df.iterrows():
            if not row["offer_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π offer_id",
                    row_number=idx + 2
                ))
            if not row["category_id"]:
                result.add_issue(Issue(
                    type=IssueType.EMPTY_ID,
                    severity="warning",
                    message="–ü—É—Å—Ç–æ–π category_id",
                    row_number=idx + 2
                ))
        
        df = df[(df["offer_id"] != "") & (df["category_id"] != "")]
        
        result.counts["total_rows"] = total_rows
        result.counts["valid_rows"] = len(df)
        
        if df.empty:
            result.msg = "–í Excel –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫"
            return result
        
        groups = df.groupby("offer_id")["category_id"].apply(list).to_dict()
        buf = io.BytesIO()
        created = 0
        added = 0
        
        with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
            for offer_id, cats in groups.items():
                cat_json = _build_category(_normalize_id(offer_id), [_normalize_id(x) for x in cats])
                zf.writestr(f"{POC_DIR}/{_safe_name(offer_id)}.json", _json_dumps_stable(cat_json))
                created += 1
                added += len(cat_json["category"])
        
        buf.seek(0)
        result.counts["created_jsons"] = created
        result.counts["categories_total"] = added
        result.ok = True
        result.msg = "–ì–æ—Ç–æ–≤–æ"
        result.zip_data = buf
        
    except Exception as e:
        result.add_issue(Issue(
            type=IssueType.INVALID_JSON,
            severity="error",
            message=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        ))
        result.msg = f"–û—à–∏–±–∫–∞: {e}"
    
    return result


# =========================
# UI –§–£–ù–ö–¶–ò–ò
# =========================
def _show_counts(counts: Dict[str, int]):
    if not counts:
        return
    items = list(counts.items())
    for i in range(0, len(items), 4):
        cols = st.columns(4)
        for j, (k, v) in enumerate(items[i:i+4]):
            with cols[j]:
                st.metric(k, v)


def _show_skipped_details(details: Optional[Dict[str, Any]], filename: str = "skipped_details.csv"):
    rows = (details or {}).get("skipped_existing") or []
    with st.expander(f"–î–µ—Ç–∞–ª–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (skipped_existing): {len(rows)}", expanded=False):
        if not rows:
            st.caption("–ù–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤.")
            return
        df = pd.DataFrame(rows)
        st.dataframe(df, use_container_width=True, height=320)
        csv_buf = io.StringIO()
        df.to_csv(csv_buf, index=False)
        st.download_button(
            "–°–∫–∞—á–∞—Ç—å –¥–µ—Ç–∞–ª–∏ (CSV)",
            csv_buf.getvalue().encode("utf-8-sig"),
            file_name=filename,
            mime="text/csv",
        )


def _show_all_issues(issues: List[Issue]):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ severity"""
    if not issues:
        st.success("‚úÖ –û—à–∏–±–æ–∫ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –Ω–µ—Ç")
        return
    
    errors = [i for i in issues if i.severity == "error"]
    warnings = [i for i in issues if i.severity == "warning"]
    infos = [i for i in issues if i.severity == "info"]
    
    # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
    col1, col2, col3 = st.columns(3)
    with col1:
        if errors:
            st.metric("üî¥ –û—à–∏–±–∫–∏", len(errors))
    with col2:
        if warnings:
            st.metric("üü° –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è", len(warnings))
    with col3:
        if infos:
            st.metric("üîµ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", len(infos))
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Å–ø–∏—Å–∫–∏
    if errors:
        with st.expander(f"üî¥ –û—à–∏–±–∫–∏ ({len(errors)})", expanded=True):
            _show_issues_table(errors)
    
    if warnings:
        with st.expander(f"üü° –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è ({len(warnings)})", expanded=False):
            _show_issues_table(warnings)
    
    if infos:
        with st.expander(f"üîµ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ({len(infos)})", expanded=False):
            _show_issues_table(infos)
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º
    _export_all_issues_csv(issues)


def _show_issues_table(issues: List[Issue]):
    """–¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–±–ª–µ–º"""
    data = []
    for issue in issues:
        row = {
            "–¢–∏–ø": issue.type.value,
            "–°–æ–æ–±—â–µ–Ω–∏–µ": issue.message,
            "–§–∞–π–ª": issue.file_path or "-",
            "–°—Ç—Ä–æ–∫–∞": issue.row_number or "-",
        }
        if issue.context:
            for k, v in issue.context.items():
                row[k] = str(v)
        data.append(row)
    
    if data:
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True, height=min(400, len(df) * 35 + 38))


def _export_all_issues_csv(issues: List[Issue]):
    """–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º –≤ CSV"""
    if not issues:
        return
    
    data = []
    for issue in issues:
        row = {
            "severity": issue.severity,
            "type": issue.type.value,
            "message": issue.message,
            "file_path": issue.file_path or "",
            "row_number": issue.row_number or "",
        }
        if issue.context:
            for k, v in issue.context.items():
                row[f"context_{k}"] = str(v)
        data.append(row)
    
    df = pd.DataFrame(data)
    csv_buf = io.StringIO()
    df.to_csv(csv_buf, index=False)
    
    st.download_button(
        "–°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç (CSV)",
        csv_buf.getvalue().encode("utf-8-sig"),
        file_name="full_issues_report.csv",
        mime="text/csv",
    )


# =========================
# STREAMLIT UI
# =========================
st.set_page_config(page_title="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ JSON", layout="wide", initial_sidebar_state="expanded")

st.title("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ JSON")
st.caption("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∞–º–∏ (AddOns), –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ —Ç–∞—Ä–∏—Ñ–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏")

st.sidebar.title("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
main_section = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª:", ["–£—Å–ª—É–≥–∏ (AddOns)", "–ü–µ—Ä–µ—Ö–æ–¥—ã —Ç–∞—Ä–∏—Ñ–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤", "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏"])

# --------- –†–∞–∑–¥–µ–ª 1: –£—Å–ª—É–≥–∏ ----------
if main_section == "–£—Å–ª—É–≥–∏ (AddOns)":
    st.header("–†–∞–±–æ—Ç–∞ —Å —É—Å–ª—É–≥–∞–º–∏")
    scenario = st.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏—é:",
        [
            "1. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É—Å–ª—É–≥–∏ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞—Ä–∏—Ñ–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤",
            "2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–ª–∞–Ω—ã",
            "3. –≠–∫—Å–ø–∞–π—Ä —É—Å–ª—É–≥–∏",
            "4. –≠–∫—Å–ø–∞–π—Ä + –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏"
        ]
    )

    if scenario.startswith("1."):
        st.subheader("–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É—Å–ª—É–≥–∏ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞—Ä–∏—Ñ–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤")
        st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: Addons name, Addons ID, –ò–º—è —É—Å–ª—É–≥–∏, ID —É—Å–ª—É–≥–∏")
        excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV", type=["xlsx", "xls", "csv"])
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
            if not excel_file:
                st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = generate_addon_from_excel(excel_file.read())
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    if res.zip_data:
                        st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "addons.zip", "application/zip")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

    elif scenario.startswith("2."):
        st.subheader("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–ª–∞–Ω—ã")
        st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: Addons ID, –ò–º—è —É—Å–ª—É–≥–∏, ID —É—Å–ª—É–≥–∏")
        zip_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP —Å –ø–ª–∞–Ω–∞–º–∏", type=["zip"])
        excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV —Å —É—Å–ª—É–≥–∞–º–∏", type=["xlsx", "xls", "csv"])
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
            if not zip_file or not excel_file:
                st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∏ Excel/CSV")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = add_services_to_existing_pogs(zip_file.read(), excel_file.read())
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    _show_skipped_details(res.details, filename="skipped_services_existing.csv")
                    if res.zip_data:
                        st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "updated_addons.zip", "application/zip")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

    elif scenario.startswith("3."):
        st.subheader("–≠–∫—Å–ø–∞–π—Ä —É—Å–ª—É–≥–∏")
        st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: json_id, service_id")
        zip_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP —Å –ø–ª–∞–Ω–∞–º–∏", type=["zip"])
        excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV —Å–æ —Å–ø–∏—Å–∫–æ–º –∫ —ç–∫—Å–ø–∞–π—Ä—É", type=["xlsx", "xls", "csv"])
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
            if not zip_file or not excel_file:
                st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∏ Excel/CSV")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = expire_services_in_pogs(zip_file.read(), excel_file.read())
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    if res.zip_data:
                        st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "expired_addons.zip", "application/zip")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

    else:  # 4. –≠–∫—Å–ø–∞–π—Ä + –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏
        st.subheader("–≠–∫—Å–ø–∞–π—Ä + –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥–∏")
        st.info("""
        **–î–≤–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:**
        1. –≠–∫—Å–ø–∞–π—Ä —É—Å–ª—É–≥ –∏–∑ —Ñ–∞–π–ª–∞ 1 (–≥–¥–µ –æ–Ω–∏ –Ω–∞–π–¥–µ–Ω—ã)
        2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª—É–≥ –∏–∑ —Ñ–∞–π–ª–∞ 2 (–≤–æ –≤—Å–µ JSON)
        
        –û–±–∞ —Ñ–∞–π–ª–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: **ID —É—Å–ª—É–≥–∏, –ò–º—è —É—Å–ª—É–≥–∏**
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("##### üìÅ –§–∞–π–ª—ã –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞")
            zip_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP —Å –ø–ª–∞–Ω–∞–º–∏", type=["zip"], key="expire_add_zip")
            expire_file = st.file_uploader(
                "Excel/CSV —Å–æ —Å–ø–∏—Å–∫–æ–º —É—Å–ª—É–≥ –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞",
                type=["xlsx", "xls", "csv"],
                key="expire_file"
            )
        
        with col2:
            st.markdown("##### üìÅ –§–∞–π–ª—ã –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è")
            st.write("")  # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
            st.write("")
            add_file = st.file_uploader(
                "Excel/CSV —Å–æ —Å–ø–∏—Å–∫–æ–º —É—Å–ª—É–≥ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è",
                type=["xlsx", "xls", "csv"],
                key="add_file"
            )
        
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å", type="primary"):
            if not zip_file or not expire_file or not add_file:
                st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ —Ç—Ä–∏ —Ñ–∞–π–ª–∞")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = expire_and_add_services(
                        zip_file.read(),
                        expire_file.read(),
                        add_file.read()
                    )
                
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    
                    # –î–µ—Ç–∞–ª–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤
                    if res.details:
                        col1, col2 = st.columns(2)
                        with col1:
                            expire_skipped = res.details.get("skipped_expire_not_found", [])
                            with st.expander(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è —ç–∫—Å–ø–∞–π—Ä–∞: {len(expire_skipped)}", expanded=False):
                                if expire_skipped:
                                    df = pd.DataFrame(expire_skipped)
                                    st.dataframe(df, use_container_width=True)
                        
                        with col2:
                            add_skipped = res.details.get("skipped_add_existing", [])
                            with st.expander(f"‚ö†Ô∏è –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç: {len(add_skipped)}", expanded=False):
                                if add_skipped:
                                    df = pd.DataFrame(add_skipped)
                                    st.dataframe(df, use_container_width=True)
                    
                    if res.zip_data:
                        st.download_button(
                            "–°–∫–∞—á–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π ZIP",
                            res.zip_data,
                            "expire_and_add_services.zip",
                            "application/zip"
                        )
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

# --------- –†–∞–∑–¥–µ–ª 2: –ü–µ—Ä–µ—Ö–æ–¥—ã ----------
elif main_section == "–ü–µ—Ä–µ—Ö–æ–¥—ã —Ç–∞—Ä–∏—Ñ–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤":
    st.header("–†–∞–±–æ—Ç–∞ —Å –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ (replaceOffer)")
    scenario = st.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏—é:",
        [
            "1. –°–æ–∑–¥–∞—Ç—å –ø–µ—Ä–µ—Ö–æ–¥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∞—Ä–∏—Ñ–Ω–æ–≥–æ –ø–ª–∞–Ω–∞",
            "2. –î–æ–±–∞–≤–∏—Ç—å —Ç–∞—Ä–∏—Ñ –≤ –ø–µ—Ä–µ—Ö–æ–¥—ã",
            "3. –≠–∫—Å–ø–∞–π—Ä —Ç–∞—Ä–∏—Ñ–∞ –≤ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö"
        ]
    )

    if scenario.startswith("1."):
        st.subheader("–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –ø–µ—Ä–µ—Ö–æ–¥")
        st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü: offer_id")
        excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV —Å offer_id", type=["xlsx", "xls", "csv"])
        col1, col2 = st.columns(2)
        with col1:
            json_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–∞", placeholder="Replace for ...")
        with col2:
            json_id = st.text_input("ID –ø–µ—Ä–µ—Ö–æ–¥–∞")
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
            if not excel_file or not json_name or not json_id:
                st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = create_replace_offer_from_excel(excel_file.read(), json_name, json_id)
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    if res.zip_data:
                        st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "replace_offer.zip", "application/zip")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

    elif scenario.startswith("2."):
        st.subheader("–î–æ–±–∞–≤–∏—Ç—å —Ç–∞—Ä–∏—Ñ –≤ –ø–µ—Ä–µ—Ö–æ–¥—ã")
        st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü: json_id (ID –ø–µ—Ä–µ—Ö–æ–¥–∞)")
        zip_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP —Å –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏", type=["zip"])
        excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV —Å–æ —Å–ø–∏—Å–∫–æ–º –ø–µ—Ä–µ—Ö–æ–¥–æ–≤", type=["xlsx", "xls", "csv"])
        offer_id = st.text_input("ID —Ç–∞—Ä–∏—Ñ–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ (offer_id)")
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
            if not zip_file or not excel_file or not offer_id:
                st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = add_offer_to_transitions(zip_file.read(), excel_file.read(), offer_id)
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    _show_skipped_details(res.details, filename="skipped_offers_existing.csv")
                    if res.zip_data:
                        st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "updated_replace_offers.zip", "application/zip")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

    else:
        st.subheader("–≠–∫—Å–ø–∞–π—Ä —Ç–∞—Ä–∏—Ñ–∞ –≤ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö")
        st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: json_id, offer_id")
        zip_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP —Å –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏", type=["zip"])
        excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV", type=["xlsx", "xls", "csv"])
        if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
            if not zip_file or not excel_file:
                st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∏ Excel/CSV")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                    res = expire_offer_in_transitions(zip_file.read(), excel_file.read())
                if not res.ok:
                    st.error(res.msg)
                else:
                    st.success(res.msg)
                    _show_counts(res.counts)
                    if res.zip_data:
                        st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "expired_replace_offers.zip", "application/zip")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
                if res.issues:
                    st.markdown("---")
                    _show_all_issues(res.issues)

# --------- –†–∞–∑–¥–µ–ª 3: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ ----------
else:
    st.header("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (ProductOfferingCategory)")
    st.subheader("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ Excel/CSV")
    st.info("Excel/CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: offer_id, category_id (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω offer_id –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è)")
    excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV", type=["xlsx", "xls", "csv"])
    if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å"):
        if not excel_file:
            st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV")
        else:
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                res = generate_categories_from_excel(excel_file.read())
            if not res.ok:
                st.error(res.msg)
            else:
                st.success(res.msg)
                _show_counts(res.counts)
                if res.zip_data:
                    st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", res.zip_data, "categories.zip", "application/zip")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
            if res.issues:
                st.markdown("---")
                _show_all_issues(res.issues)
